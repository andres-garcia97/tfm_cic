{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Dimensionality Reduction\n",
    "\n",
    "    Se pretende en este notebook mostrar una comparativa en términos de resultados\n",
    "    de la reducción dimensional que hacen dos métodos sobre el mismo conjunto de datos:\n",
    "    PCA vs Auto-Encoders.\n",
    "\n",
    "\"\"\"\n",
    "## LIBRARIES AND DATA\n",
    "\n",
    "# Generic libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "\n",
    "from datetime import datetime as dt\n",
    "from datetime import date\n",
    "from datetime import timedelta\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow import keras\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DATA EXTRACTION AND PREPARATION\n",
    "\n",
    "# Numeric values extraction (excel LVSM_Def.xlsx)\n",
    "values_column_names = [\"time\", \"branch\" , \"organization\", \"substation\", \"transformer_code\", \"App SW\", \n",
    "                        \"V_L1\", \"I_L1\", \"W_L1\", \"QL_L1\", \"QC_L1\",\"cos_L1\", \"angle_L1\",\n",
    "                        \"V_L2\", \"I_L2\", \"W_L2\", \"QL_L2\", \"QC_L2\",\"cos_L2\", \"angle_L2\",\n",
    "                        \"V_L3\", \"I_L3\", \"W_L3\", \"QL_L3\", \"QC_L3\",\"cos_L3\", \"angle_L3\",\n",
    "                        \"temp_amb\",\n",
    "                        \"aplus_L1\", \"aminus_L1\", \"RplusL_L1\", \"RminusL_L1\", \"RplusC_L1\", \"RminusC_L1\", \n",
    "                        \"aplus_L2\", \"aminus_L2\", \"RplusL_L2\", \"RminusL_L2\", \"RplusC_L2\", \"RminusC_L2\",\n",
    "                        \"aplus_L3\", \"aminus_L3\", \"RplusL_L3\", \"RminusL_L3\", \"RplusC_L3\", \"RminusC_L3\"]\n",
    "\n",
    "# Retrieve data on values\n",
    "# script_path = os.path.dirname(__file__)\n",
    "# Read csv from local file\n",
    "data_lvsm = pd.read_csv('../DATA/LVSM_Def.csv',  sep = ';', header=0, names=values_column_names)\n",
    "\n",
    "# Read csv from GitHub\n",
    "# url_data = 'https://gitlab.com/Ander_gargas/tfm-cic/-/raw/master/Listado_Trafos.csv'\n",
    "# data_lvsm = pd.read_csv(url_data,  sep = ';', header=0, names=values_column_names, encoding='latin-1')\n",
    "\n",
    "# Cleaning data table\n",
    "data = data_lvsm.drop([\"aplus_L1\", \"aminus_L1\", \"RplusL_L1\", \"RminusL_L1\", \"RplusC_L1\", \"RminusC_L1\", \n",
    "                  \"aplus_L2\", \"aminus_L2\", \"RplusL_L2\", \"RminusL_L2\", \"RplusC_L2\", \"RminusC_L2\",\n",
    "                  \"aplus_L3\", \"aminus_L3\", \"RplusL_L3\", \"RminusL_L3\", \"RplusC_L3\", \"RminusC_L3\"], axis=1)\n",
    "data = data.reset_index(drop = True)\n",
    "\n",
    "# Change column types to appropiate\n",
    "data = data.astype({\"time\": str, \"branch\": str , \"organization\": str, \"substation\": str, \"transformer_code\": str, \"App SW\": str})\n",
    "\n",
    "data[[\"V_L1\", \"I_L1\", \"W_L1\", \"QL_L1\", \"QC_L1\",\"cos_L1\", \"angle_L1\",\n",
    "      \"V_L2\", \"I_L2\", \"W_L2\", \"QL_L2\", \"QC_L2\",\"cos_L2\", \"angle_L2\",\n",
    "      \"V_L3\", \"I_L3\", \"W_L3\", \"QL_L3\", \"QC_L3\",\"cos_L3\", \"angle_L3\",\n",
    "      \"temp_amb\"]] = data[[\"V_L1\", \"I_L1\", \"W_L1\", \"QL_L1\", \"QC_L1\",\"cos_L1\", \"angle_L1\",\n",
    "                           \"V_L2\", \"I_L2\", \"W_L2\", \"QL_L2\", \"QC_L2\",\"cos_L2\", \"angle_L2\",\n",
    "                           \"V_L3\", \"I_L3\", \"W_L3\", \"QL_L3\", \"QC_L3\",\"cos_L3\", \"angle_L3\",\n",
    "                           \"temp_amb\"]].astype(float)\n",
    "\n",
    "### Deal with the \"24:00\" problem. Adapt BOTH the hour and the day. \n",
    "# Get the indexes and replace hour\n",
    "for i, date in enumerate(data['time']):\n",
    "    if date.split()[1].split(':')[0] == '24':\n",
    "        data.loc[i, 'time'] = data.loc[i, 'time'].replace(\"24:00\",\"00:00\")\n",
    "        data.loc[i, 'time'] = pd.to_datetime(data.loc[i, 'time'], format = '%Y-%m-%d %H:%M') + timedelta(days = 1)\n",
    "\n",
    "# Update the format\n",
    "data['time'] = pd.to_datetime(data['time'], format = '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Copy of the dataframe to split date and hour\n",
    "data_new = data.copy(deep=True)\n",
    "\n",
    "# Split the time column into date and hour columns, for diagram's input preparation\n",
    "data_new['date'] = (data_new['time']).dt.date\n",
    "data_new['hour'] = (data_new['time']).dt.time\n",
    "\n",
    "# Delete the old time column\n",
    "data_new = data_new.drop([\"time\"], axis=1)\n",
    "\n",
    "# Put both columns at the start\n",
    "data_new = pd.concat([data_new['hour'], data_new.drop('hour',axis=1)], axis=1)\n",
    "data_new = pd.concat([data_new['date'], data_new.drop('date',axis=1)], axis=1)\n",
    "\n",
    "# Cleaning NA values\n",
    "if data_new.isna().sum().sum() < .10 * len(data_new): \n",
    "    # print (\"Cleaning NA values from dataset\")\n",
    "    data_new = data_new.dropna()\n",
    "else:\n",
    "    raise Exception(\"Careful! Deleting NaN values would cut most of the dataset\")\n",
    "\n",
    "# Remove duplicates\n",
    "if data.duplicated().sum() < .10 * len(data_new): \n",
    "    # print (\"Cleaning duplicate values from dataset\")\n",
    "    data_new = data_new.drop_duplicates(subset=['date', 'hour', 'substation', 'App SW'])\n",
    "else:\n",
    "    raise Exception(\"Careful! Deleting duplicated values would cut most of the dataset\")\n",
    "\n",
    "# Prepare the train (learn), validation (optimize) and test dataset (classification performances)\n",
    "msk = np.random.rand(len(data_new)) < 0.98\n",
    "\n",
    "df_train = data_new[msk]\n",
    "df_test = data_new[~msk]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Part 1: PCA analysis\n",
    "\n",
    "\"\"\"\n",
    "Principal Component Analysis (PCA) is one of the most popular dimensionality reduction algorithms. \n",
    "PCA works by finding the axes that account for the larges amount of variance in the data which are orthogonal to each other.\n",
    "\n",
    "The steps to perform PCA are:\n",
    "    - Standardize the data.\n",
    "    - Obtain the Eigenvectors and Eigenvalues from the covariance matrix or correlation matrix, or perform Singular Value Decomposition.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Part 2: Simple undercomplete AE\n",
    "\n",
    "\"\"\"\n",
    "An Autoencoder (AE) on the other hand is a special kind of neural network which is trained to copy its input to its output. \n",
    "First, it maps the input to a latent space of reduced dimension, then code back the latent representation to the output.\n",
    "An AE learns to compress data by reducing the reconstruction error.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Part 3: Stacked Linear AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PArt 4: Non-Linear AE"
   ]
  }
 ]
}